{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q 2 B. NEURAL NETWORK with \n",
    "### 30 Hidden Sigmoid Nodes , \n",
    "### 2 HIdden Layers , \n",
    "### with and without L2 regularisations and \n",
    "### 10 Softmax output nodes \n",
    "## -------------COMMON CODE STARTS ----------------##\n",
    "### - Import Libraries including TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## -Read the MNIST Train and Test Data\n",
    "### - Pick 100 Samples of each digit for Train and Test \n",
    "### - Convert the Labels into One Hot Encoded for Softmax Output calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GETTING 1000 samples\n",
      "GETTING 1000 samples\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('/Users/vikrant/GoogleDrive/CSE555-Intro2Pattern-WenDong/HW-45/mnist_train_nn1.csv')\n",
    "datatest=pd.read_csv('/Users/vikrant/GoogleDrive/CSE555-Intro2Pattern-WenDong/HW-45/mnist_test_nn1.csv')\n",
    "\n",
    "tmp=pd.DataFrame()\n",
    "        \n",
    "print(\"GETTING 1000 samples\")\n",
    "for i in range(10):\n",
    "    loop = data.iloc[np.where(data.iloc[:,0] == i)]\n",
    "            \n",
    "            #https://stackoverflow.com/questions/29576430/shuffle-dataframe-rows\n",
    "            #https://stackoverflow.com/questions/15923826/random-row-selection-in-pandas-dataframe\n",
    "    loop1 = loop.sample(n=100).reset_index(drop=True)\n",
    "            \n",
    "            #print(\"No. of ROws, columns:\" , len(loop1), \"for digit:\",i)\n",
    "    tmp = tmp.append(loop1, ignore_index = True)\n",
    "\n",
    "    \n",
    "tmptest=pd.DataFrame()\n",
    "        \n",
    "print(\"GETTING 1000 samples\")\n",
    "for i in range(10):\n",
    "    loop = datatest.iloc[np.where(datatest.iloc[:,0] == i)]\n",
    "            \n",
    "            #https://stackoverflow.com/questions/29576430/shuffle-dataframe-rows\n",
    "            #https://stackoverflow.com/questions/15923826/random-row-selection-in-pandas-dataframe\n",
    "    loop1 = loop.sample(n=100).reset_index(drop=True)\n",
    "            \n",
    "            #print(\"No. of ROws, columns:\" , len(loop1), \"for digit:\",i)\n",
    "    tmptest = tmptest.append(loop1, ignore_index = True)\n",
    "\n",
    "    \n",
    "tmp1=tmp.values\n",
    "tmptest1=tmptest.values\n",
    "\n",
    "np.random.shuffle(tmp1)\n",
    "np.random.shuffle(tmptest1)\n",
    "\n",
    "\n",
    "images = tmp1[:,1:]\n",
    "#images = np.multiply(images, 1.0 / 255.0)\n",
    "imagestest = tmptest1[:,1:]\n",
    "#imagestest = np.multiply(imagestest, 1.0 / 255.0)\n",
    "#images = images.astype(np.float64)\n",
    "labls = tmp1[:,0]\n",
    "lablstest = tmptest1[:,0]\n",
    "\n",
    "labels=labls.reshape(len(labls),1)\n",
    "labelstest=lablstest.reshape(len(lablstest),1)\n",
    "\n",
    "def dense_to_one_hot(labels_dense, num_classes):\n",
    "    num_labels = labels_dense.shape[0]\n",
    "    index_offset = np.arange(num_labels) * num_classes\n",
    "    labels_one_hot = np.zeros((num_labels, num_classes))\n",
    "    labels_one_hot.flat[index_offset + labels_dense.ravel()] = 1\n",
    "    return labels_one_hot\n",
    "\n",
    "labels = dense_to_one_hot(labls, 10)\n",
    "labels = labels.astype(np.uint8)\n",
    "labelstest = dense_to_one_hot(lablstest, 10)\n",
    "labelstest = labelstest.astype(np.uint8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - Function to pick batch size 10 data from Training and Test dataset for Tensor Flow Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_completed = 0\n",
    "index_in_epoch = 0\n",
    "num_examples = images.shape[0]\n",
    "\n",
    "# serve data by batches\n",
    "def next_batch(batch_size):\n",
    "    \n",
    "    global images\n",
    "    global labels\n",
    "    global index_in_epoch\n",
    "    global epochs_completed\n",
    "    \n",
    "    start = index_in_epoch\n",
    "    index_in_epoch += batch_size\n",
    "    \n",
    "    # when all trainig data have been already used, it is reorder randomly    \n",
    "    if index_in_epoch > num_examples:\n",
    "        # finished epoch\n",
    "        epochs_completed += 1\n",
    "        # shuffle the data\n",
    "        perm = np.arange(num_examples)\n",
    "        np.random.shuffle(perm)\n",
    "        images = images[perm]\n",
    "        labels = labels[perm]\n",
    "        # start next epoch\n",
    "        start = 0\n",
    "        index_in_epoch = batch_size\n",
    "        assert batch_size <= num_examples\n",
    "    end = index_in_epoch\n",
    "    return images[start:end], labels[start:end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_completed = 0\n",
    "index_in_epoch = 0\n",
    "num_examples = images.shape[0]\n",
    "\n",
    "# serve data by batches\n",
    "def next_batch_Te(batch_size):\n",
    "    \n",
    "    global imagestest\n",
    "    global labelstest\n",
    "    global index_in_epoch\n",
    "    global epochs_completed\n",
    "    \n",
    "    start = index_in_epoch\n",
    "    index_in_epoch += batch_size\n",
    "    \n",
    "    # when all trainig data have been already used, it is reorder randomly    \n",
    "    if index_in_epoch > num_examples:\n",
    "        # finished epoch\n",
    "        epochs_completed += 1\n",
    "        # shuffle the data\n",
    "        perm = np.arange(num_examples)\n",
    "        np.random.shuffle(perm)\n",
    "        images = images[perm]\n",
    "        labels = labels[perm]\n",
    "        # start next epoch\n",
    "        start = 0\n",
    "        index_in_epoch = batch_size\n",
    "        assert batch_size <= num_examples\n",
    "    end = index_in_epoch\n",
    "    return imagestest[start:end], labelstest[start:end]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## -------------COMMON CODE ENDS ----------------##\n",
    "# - TF Code to run Neural Network for \"Problem 2B\" with below paramters\n",
    "## - 30 Hidden Sigmoid Nodes, 2 Hidden Layers, No L2 Regularisation and 10 Softmax output nodes \n",
    "### - Learning Rate=0.1, Number of Epochs= 30, Batch Size = 10\n",
    "### - Use the scoping of all TF Parameters (Arrays and Scalar) for Display in Tensor Board"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "softmax: Tensor(\"Output_Layer/Softmax:0\", shape=(?, 10), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.1\n",
    "training_epochs = 30\n",
    "batch_size = 10\n",
    "display_step = 2\n",
    "beta = 5\n",
    "logdir = '/Users/vikrant/GoogleDrive/CSE555-Intro2Pattern-WenDong/HW-45/'\n",
    "with tf.name_scope('input'):#as scope:\n",
    "    # None -> batch size can be any size, 784 -> flattened mnist image\n",
    "   \n",
    "    x = tf.placeholder(tf.float32, shape=[None, 784], name=\"Input\") \n",
    "with tf.name_scope('output'):#as scope:    \n",
    "    # target 10 output classes\n",
    "    y_ = tf.placeholder(tf.float32, shape=[None, 10], name=\"Output\")\n",
    "\n",
    "# Create a model\n",
    "with tf.name_scope(\"Hidden_Layer\"):# as scope:\n",
    "    W1 = tf.Variable(tf.random_normal([784, 30], stddev=0.03), name='W1_weights')\n",
    "    b1 = tf.Variable(tf.random_normal([30]), name='b1_biases')\n",
    "    \n",
    "    W1_h = tf.summary.histogram(\"W1_weights\", W1)\n",
    "    b1_h = tf.summary.histogram(\"b1_biases\", b1)\n",
    "    \n",
    "    hidden_out = tf.add(tf.matmul(x,W1),b1)\n",
    "    hidden_out = tf.nn.sigmoid(hidden_out, name = \"H_Activation\")\n",
    "    hidden_h = tf.summary.histogram(\"Hidden_Activation\", hidden_out)\n",
    "\n",
    "with tf.name_scope('Hidden_Layer2'):\n",
    "    Wh2 = tf.Variable(tf.random_normal([30, 30], stddev=0.03), name='Wh2_weights')#Between Hidden Layer1&2\n",
    "    bh2 = tf.Variable(tf.random_normal([30]), name='bh2_biases')\n",
    "                      \n",
    "    Wh2_h = tf.summary.histogram(\"Wh2_weights\", Wh2)\n",
    "    bh2_h = tf.summary.histogram(\"bh2_biases\", bh2)\n",
    "                      \n",
    "    hidden2_out = tf.add(tf.matmul(hidden_out,Wh2),bh2)\n",
    "    hidden2_out = tf.nn.sigmoid(hidden2_out)\n",
    "    hidden_h = tf.summary.histogram(\"Hidden2_Activation\", hidden2_out)                  \n",
    "                      \n",
    "with tf.name_scope(\"Output_Layer\"):# as scope:\n",
    "    W2 = tf.Variable(tf.random_normal([30, 10], stddev=0.03), name='W2_weights')# and the weights connecting the hidden layer to the output layer\n",
    "    b2 = tf.Variable(tf.random_normal([10]), name='b2_biases')\n",
    "    W2_h = tf.summary.histogram(\"W2_weights\", W2)\n",
    "    b2_h = tf.summary.histogram(\"b2_biases\", b2)\n",
    "    \n",
    "    logits = tf.matmul(hidden2_out,W2) + b2\n",
    "    y = tf.nn.softmax(logits)\n",
    "    softmax_h = tf.summary.histogram(\"Softmax_Activation\", y)\n",
    "    print(\"softmax:\", y)\n",
    "\n",
    "with tf.name_scope(\"optimiser\"):#as scope:\n",
    "    with tf.name_scope(\"Error\"):\n",
    "        #error_loss= tf.reduce_mean(tf.squared_difference(y, y_))\n",
    "        error_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y_))#,reduction_indices=[1] )\n",
    "        #regularizers = tf.nn.l2_loss(W1) + tf.nn.l2_loss(W2)\n",
    "        error_loss = tf.reduce_mean(error_loss)# + beta * regularizers) \n",
    "        erro_h= tf.summary.histogram(\"Error\", error_loss)\n",
    "        #tf.summary.scalar(\"regularizer\", regularizers)\n",
    "        tf.summary.scalar(\"cost\", error_loss)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(error_loss)\n",
    "    #optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(error_loss)\n",
    "# define an accuracy assessment operation\n",
    "with tf.name_scope(\"Accuracy\"):#as scope:\n",
    "    correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    accuracy_h =tf.summary.histogram(\"Accuracy\", accuracy)\n",
    "    tf.summary.scalar(\"accuracy_train\", accuracy)\n",
    "    \n",
    "#with tf.name_scope(\"Learning_rate\"):\n",
    "   #p_lr = tf.placeholder(tf.float32, (), name='learning_rate')\n",
    "    \n",
    "    \n",
    "\n",
    "test_summary = tf.summary.scalar(\"test_accuracy\", accuracy)\n",
    "\n",
    "summary_op = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0\n",
      "Epoch:  5\n",
      "Epoch:  10\n",
      "Epoch:  15\n",
      "Epoch:  20\n",
      "Epoch:  25\n",
      "Accuracy:  0.274\n",
      "0.274\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    # variables need to be initialized before we can use them\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    # create log writer object\n",
    "    train_writer = tf.summary.FileWriter(logdir+'/tflog_pcip_h2_ce_woL2'+ '/train', graph=tf.get_default_graph())\n",
    "    test_writer = tf.summary.FileWriter(logdir+'/tflog_pcip_h2_ce_woL2'+ '/test', graph=tf.get_default_graph())    \n",
    "    # perform training cycles\n",
    "    # create log writer object\n",
    "    #writer = tf.summary.FileWriter('/Users/vikrant/GoogleDrive/CSE555-Intro2Pattern-WenDong/HW-45/tflogsAdam/5', graph=tf.get_default_graph())\n",
    "        \n",
    "    # perform training cycles\n",
    "    for iteration in range(training_epochs):\n",
    "        \n",
    "        # number of batches in one epoch\n",
    "        #batch_count = int(len(y_train)/batch_size)\n",
    "        batch_count = int(len(labels)/batch_size)\n",
    "        for i in range(batch_count):\n",
    "            #batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "            #batch_x, batch_y = images[i:i+10], labels[i:i+10]\n",
    "            batch_x, batch_y = next_batch(batch_size)\n",
    "            #batch_x, batch_y = x_train[i:i+10], y_train[i:i+10]\n",
    "            #batch_x=formatdata(batch_x)\n",
    "            #show_digits(batch_x, batch_y, \"Digits{}\")\n",
    "            #batch_xs, batch_ys = x_test[i:i+10], y_test[i:i+10]\n",
    "            batch_xs, batch_ys = next_batch_Te(batch_size)\n",
    "            #batch_xs=formatdata(batch_xs)\n",
    "            # perform the operations we defined earlier on batch\n",
    "            _, summary = sess.run([optimizer, summary_op], feed_dict={x: batch_x, y_: batch_y})\n",
    "           # _, l, a = sess.run([optimizer, error_loss, accuracy], feed_dict={x: batch_x, y_: batch_y})\n",
    "           # _, test_summary = sess.run([optimizer, summary_op], feed_dict={x: batch_xs, y_: batch_ys})\n",
    "            summary_test, acc = sess.run([summary_op, accuracy], feed_dict={x: batch_xs, y_: batch_ys})\n",
    "           \n",
    "            train_writer.add_summary(summary, iteration * batch_count + i)\n",
    "            test_writer.add_summary(summary_test, iteration * batch_count + i)\n",
    "            # write log\n",
    "            #print(\"iteration:\", iteration, \"cost:\",l,\"train_accuracy:\", a)\n",
    "            #writer.add_summary(summary, iteration * batch_count + i)\n",
    "            #writer.add_summary(test_summary,iteration * batch_count + i)\n",
    "            #print(\"iteration * batch_count + i:\", iteration * batch_count + i)\n",
    "        if iteration % 5 == 0: \n",
    "            print (\"Epoch: \", iteration) \n",
    "    #print (\"Accuracy: \", accuracy.eval(feed_dict={x: x_test, y_: y_test}))\n",
    "    print (\"Accuracy: \", accuracy.eval(feed_dict={x: imagestest, y_: labelstest}))\n",
    "    print(sess.run(accuracy, feed_dict={x: imagestest, y_: labelstest}))\n",
    "    #print(sess.run(accuracy, feed_dict={x: x_test, y_: y_test}))\n",
    "    print (\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -(For Graphs Please see the Report)-###\n",
    "# -----------------------------------------------------------------------------#\n",
    "# Q 2 B. NEURAL NETWORK with 30 Hidden Sigmoid Nodes, L2 Regularisation and 10 Softmax output nodes \n",
    "# -----------------------------------------------------------------------------#\n",
    "### - Import Libraries including TensorFlow\n",
    "## - TF Code to run Neural Network for \"problem 2B\" with below paramters\n",
    "### - 30 Hidden Sigmoid Nodes, 2 Hidden Nodes , L2 Regularisation and 10 Softmax output nodes\n",
    "### - Learning Rate=0.1, Number of Epochs= 30, Batch Size = 10, L2 Regularisation = 5\n",
    "### - Use the scoping of all TF Parameters (Arrays and Scalar) for Display in Tensor Board"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ERROR Fn. CROSS ENTROPY,H2, L2 Regularisation, I/P: MyPC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "softmax: Tensor(\"Output_Layer/Softmax:0\", shape=(?, 10), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.1\n",
    "training_epochs = 30\n",
    "batch_size = 10\n",
    "display_step = 2\n",
    "beta = 5\n",
    "logdir = '/Users/vikrant/GoogleDrive/CSE555-Intro2Pattern-WenDong/HW-45/'\n",
    "with tf.name_scope('input'):#as scope:\n",
    "    # None -> batch size can be any size, 784 -> flattened mnist image\n",
    "   \n",
    "    x = tf.placeholder(tf.float32, shape=[None, 784], name=\"Input\") \n",
    "with tf.name_scope('output'):#as scope:    \n",
    "    # target 10 output classes\n",
    "    y_ = tf.placeholder(tf.float32, shape=[None, 10], name=\"Output\")\n",
    "\n",
    "# Create a model\n",
    "with tf.name_scope(\"Hidden_Layer\"):# as scope:\n",
    "    W1 = tf.Variable(tf.random_normal([784, 30], stddev=0.03), name='W1_weights')\n",
    "    b1 = tf.Variable(tf.random_normal([30]), name='b1_biases')\n",
    "    \n",
    "    W1_h = tf.summary.histogram(\"W1_weights\", W1)\n",
    "    b1_h = tf.summary.histogram(\"b1_biases\", b1)\n",
    "    \n",
    "    hidden_out = tf.add(tf.matmul(x,W1),b1)\n",
    "    hidden_out = tf.nn.sigmoid(hidden_out, name = \"H_Activation\")\n",
    "    hidden_h = tf.summary.histogram(\"Hidden_Activation\", hidden_out)\n",
    "\n",
    "with tf.name_scope('Hidden_Layer2'):\n",
    "    Wh2 = tf.Variable(tf.random_normal([30, 30], stddev=0.03), name='Wh2_weights')#Between Hidden Layer1&2\n",
    "    bh2 = tf.Variable(tf.random_normal([30]), name='bh2_biases')\n",
    "                      \n",
    "    Wh2_h = tf.summary.histogram(\"Wh2_weights\", Wh2)\n",
    "    bh2_h = tf.summary.histogram(\"bh2_biases\", bh2)\n",
    "                      \n",
    "    hidden2_out = tf.add(tf.matmul(hidden_out,Wh2),bh2)\n",
    "    hidden2_out = tf.nn.sigmoid(hidden2_out)\n",
    "    hidden_h = tf.summary.histogram(\"Hidden2_Activation\", hidden2_out)                  \n",
    "                      \n",
    "with tf.name_scope(\"Output_Layer\"):# as scope:\n",
    "    W2 = tf.Variable(tf.random_normal([30, 10], stddev=0.03), name='W2_weights')# and the weights connecting the hidden layer to the output layer\n",
    "    b2 = tf.Variable(tf.random_normal([10]), name='b2_biases')\n",
    "    W2_h = tf.summary.histogram(\"W2_weights\", W2)\n",
    "    b2_h = tf.summary.histogram(\"b2_biases\", b2)\n",
    "    \n",
    "    logits = tf.matmul(hidden2_out,W2) + b2\n",
    "    y = tf.nn.softmax(logits)\n",
    "    softmax_h = tf.summary.histogram(\"Softmax_Activation\", y)\n",
    "    print(\"softmax:\", y)\n",
    "\n",
    "with tf.name_scope(\"optimiser\"):#as scope:\n",
    "    with tf.name_scope(\"Error\"):\n",
    "        #error_loss= tf.reduce_mean(tf.squared_difference(y, y_))\n",
    "        error_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y_))#,reduction_indices=[1] )\n",
    "        regularizers = tf.nn.l2_loss(W1) + tf.nn.l2_loss(W2)\n",
    "        error_loss = tf.reduce_mean((error_loss) + beta * regularizers) \n",
    "        erro_h= tf.summary.histogram(\"Error\", error_loss)\n",
    "        tf.summary.scalar(\"regularizer\", regularizers)\n",
    "        tf.summary.scalar(\"cost\", error_loss)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(error_loss)\n",
    "    #optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(error_loss)\n",
    "# define an accuracy assessment operation\n",
    "with tf.name_scope(\"Accuracy\"):#as scope:\n",
    "    correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    accuracy_h =tf.summary.histogram(\"Accuracy\", accuracy)\n",
    "    tf.summary.scalar(\"accuracy_train\", accuracy)\n",
    "    \n",
    "#with tf.name_scope(\"Learning_rate\"):\n",
    "   #p_lr = tf.placeholder(tf.float32, (), name='learning_rate')\n",
    "    \n",
    "    \n",
    "\n",
    "test_summary = tf.summary.scalar(\"test_accuracy\", accuracy)\n",
    "\n",
    "summary_op = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0\n",
      "Epoch:  5\n",
      "Epoch:  10\n",
      "Epoch:  15\n",
      "Epoch:  20\n",
      "Epoch:  25\n",
      "Accuracy:  0.1\n",
      "0.1\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    # variables need to be initialized before we can use them\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    # create log writer object\n",
    "    train_writer = tf.summary.FileWriter(logdir+'/tflog_pcip_h2_ce_wL2'+ '/train', graph=tf.get_default_graph())\n",
    "    test_writer = tf.summary.FileWriter(logdir+'/tflog_pcip_h2_ce_wL2'+ '/test', graph=tf.get_default_graph())    \n",
    "    # perform training cycles\n",
    "    # create log writer object\n",
    "    #writer = tf.summary.FileWriter('/Users/vikrant/GoogleDrive/CSE555-Intro2Pattern-WenDong/HW-45/tflogsAdam/5', graph=tf.get_default_graph())\n",
    "        \n",
    "    # perform training cycles\n",
    "    for iteration in range(training_epochs):\n",
    "        \n",
    "        # number of batches in one epoch\n",
    "        #batch_count = int(len(y_train)/batch_size)\n",
    "        batch_count = int(len(labels)/batch_size)\n",
    "        for i in range(batch_count):\n",
    "            #batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "            #batch_x, batch_y = images[i:i+10], labels[i:i+10]\n",
    "            batch_x, batch_y = next_batch(batch_size)\n",
    "            #batch_x, batch_y = x_train[i:i+10], y_train[i:i+10]\n",
    "            #batch_x=formatdata(batch_x)\n",
    "            #show_digits(batch_x, batch_y, \"Digits{}\")\n",
    "            #batch_xs, batch_ys = x_test[i:i+10], y_test[i:i+10]\n",
    "            batch_xs, batch_ys = next_batch_Te(batch_size)\n",
    "            #batch_xs=formatdata(batch_xs)\n",
    "            # perform the operations we defined earlier on batch\n",
    "            _, summary = sess.run([optimizer, summary_op], feed_dict={x: batch_x, y_: batch_y})\n",
    "           # _, l, a = sess.run([optimizer, error_loss, accuracy], feed_dict={x: batch_x, y_: batch_y})\n",
    "           # _, test_summary = sess.run([optimizer, summary_op], feed_dict={x: batch_xs, y_: batch_ys})\n",
    "            summary_test, acc = sess.run([summary_op, accuracy], feed_dict={x: batch_xs, y_: batch_ys})\n",
    "           \n",
    "            train_writer.add_summary(summary, iteration * batch_count + i)\n",
    "            test_writer.add_summary(summary_test, iteration * batch_count + i)\n",
    "            # write log\n",
    "            #print(\"iteration:\", iteration, \"cost:\",l,\"train_accuracy:\", a)\n",
    "            #writer.add_summary(summary, iteration * batch_count + i)\n",
    "            #writer.add_summary(test_summary,iteration * batch_count + i)\n",
    "            #print(\"iteration * batch_count + i:\", iteration * batch_count + i)\n",
    "        if iteration % 5 == 0: \n",
    "            print (\"Epoch: \", iteration) \n",
    "    #print (\"Accuracy: \", accuracy.eval(feed_dict={x: x_test, y_: y_test}))\n",
    "    print (\"Accuracy: \", accuracy.eval(feed_dict={x: imagestest, y_: labelstest}))\n",
    "    print(sess.run(accuracy, feed_dict={x: imagestest, y_: labelstest}))\n",
    "    #print(sess.run(accuracy, feed_dict={x: x_test, y_: y_test}))\n",
    "    print (\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -(For Graphs Please see the Report)-###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -----------------------------------------------------------------------------#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
